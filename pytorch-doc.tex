% Created 2022-01-11 Tue 21:38
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\author{Chaitanya Kapoor}
\date{\today}
\title{Pytorch Documentation}
\hypersetup{
 pdfauthor={Chaitanya Kapoor},
 pdftitle={Pytorch Documentation},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

A list of important stuff in \textbf{PyTorch} that I do not want to keep using the official documentation for. I have made this using the book \emph{``Deep Learning with PyTorch''}.

\section{Tensor Manipulation}
\label{sec:orga93d8cf}
Tensors are indexed similar to multidimensional arrays. A Python list can also be passed to the constructor to create a tensor.
\begin{minted}[]{python}
points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])
\end{minted}
The shape of the created tensor can also be accessed. Note that \texttt{shape} is \textbf{not} a callable function.
\begin{minted}[]{python}
points.shape
\end{minted}
Some common ways to index a tensor (analogous to lists):
\begin{minted}[]{python}
points[1:]      # all rows after the first (implicitly all columns)
points[1:, :]   # all columns
points[1:, 0]   # first column
points[None]    # add a dimension of size 1 (like unsqueeze)
\end{minted}

\subsection{Tensor dimensions}
\label{sec:org6e4d5b0}
3D tensors have shapes in the following order:
\begin{minted}[]{python}
img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]
\end{minted}
A 4D image tensor has the following shape:
\begin{minted}[]{python}
batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]
\end{minted}
RGB dimensions are always counted third from the end, \textbf{-3}.

\subsection{In-place operations}
\label{sec:orgccdc2e2}
Methods which are specific to \texttt{Tensor} objects are followed by an \emph{underscore}. This is used to indicate that the method operates \emph{in place} by modifying the input instead of creating a new tensor.
\begin{minted}[]{python}
a = torch.ones(3,2)
a.zero_()
\end{minted}
This code block mutates \texttt{a} and returns a null tensor of shape \((3,2)\).

\subsection{Move to GPUs}
\label{sec:org4b63581}
A tensor can be created on the GPU by providing the following argument to the constructor:
\begin{minted}[]{python}
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')
\end{minted}
We can also copy a tensor created on the CPU to the GPU with the help of the \texttt{to} method.
\begin{minted}[]{python}
points_gpu = points.to(device='cuda')
\end{minted}
Basic tensor operations can also be transferred to the GPU rather than performing them on the CPU.
\begin{minted}[]{python}
points_gpu = 2 * points.to(device='cuda')
\end{minted}

\subsection{Serializing tensors}
\label{sec:org2c0004b}
To avoid retraining models from scratch, we can save weight tensors with the help of the \texttt{pickle} format. If we have an output file called \texttt{ourpoints.t}, then:
\begin{minted}[]{python}
torch.save(points, '../ourpoints.t')
\end{minted}
To load the weights, we call the \texttt{load} method.
\begin{minted}[]{python}
points = torch.load('..ourpoints.t')
\end{minted}

\section{Images}
\label{sec:orgeabe9b9}
\subsection{Loading images}
\label{sec:org4612643}
This can be done with the \texttt{imageio} library. Suppose that the path to the image file is \texttt{'../xyz.jpg'}:
\begin{minted}[]{python}
    import imageio
    img_arr = imageio.imread('../xyz.jpg')
\end{minted}
The image is loaded as a \texttt{NumPy} object. The layout of the dimensions is \(H\times W\times C\) corresponding to \textbf{height}, \textbf{width} and \textbf{channel} respectively.
\subsection{Changing layout}
\label{sec:org036d78a}
The layout of the image tensor can be altered using the \texttt{permute} method (as expected by \texttt{Pytorch}).
\begin{minted}[]{python}
 torch_img = torch.from_numpy(img_arr) # convert to torch tensor
 out = torch_img.permute(2, 0, 1) # (C X H X W)
\end{minted}
An extra dimension can be added for processing batches of images by specifying the dimensions of the tensor as \(N\times C\times H\times W\) where \(N\) is the batch size of the images.

\section{Learning process}
\label{sec:orgc8d1684}
\subsection{Visualizing}
\label{sec:orgdc8553c}
The most foolproof method to debug code is to begin by plotting it out. In the following code snippet, we plot \emph{raw unknown values}.
\begin{minted}[]{python}
    %matplotlib inline
    from matplotlib import pyplot as plt

    t_p = model(t_un, *params) # argument unpacking
    fig = plt.figure(dpi=600)
    plt.xlabel("Temperature (°Fahrenheit)")
    plt.ylabel("Temperature (°Celsius)")
    plt.plot(t_u.numpy(), t_p.detach().numpy())
    plt.plot(t_u.numpy(), t_c.numpy(), 'o')
\end{minted}
The argument \texttt{*params} is equivalent to passing the elements of \texttt{params} as individual arguemnts to our training model.

\subsection{Backpropagation}
\label{sec:org244c356}
A training loop can be written with the help of gradient descent optimizers and standard loss functions that are shipped along with the \texttt{PyTorch} packages. The following code block illustrates a training loop function:
\begin{minted}[]{python}
    def training_loop(n_epochs, optimizer, params, t_u, t_c):
        for epoch in range(1, n_epochs + 1):
            t_p = model(t_u, *params)
            loss = loss_fn(t_p, t_c) # custom loss function
            optimizer.zero_grad()
            loss.backward()
            optimizer.step() # update parameters
            if epoch % 500 == 0:
                print('Epoch %d, Loss %f' % (epoch, float(loss)))
        return params
\end{minted}
Note that the gradients are \emph{zeroed out} after each loss update to ensure that the gradients are not accumulated in the leaf nodes of the computation graph that is constructed. An example of invoking the above training method using the \texttt{SGD} optimizer is shown below.
\begin{minted}[]{python}
    params = torch.tensor([1.0, 0.0], requires_grad=True)
    learning_rate = 1e-2
    optimizer = optim.SGD([params], lr=learning_rate)
    training_loop(
        n_epochs = 5000,
        optimizer = optimizer,
        params = params,
        t_u = t_un,
        t_c = t_c)
\end{minted}

\subsection{Splitting datasets}
\label{sec:org7836771}
Shuffling a dataset requires us to use a random permutation of tensor indices. This is done with the help of the \texttt{randperm} function.
\begin{minted}[]{python}
    n_samples = t_u.shape[0]
    n_val = int(0.2 * n_samples)   # perform an 80:20 split
    shuffled_indices = torch.randperm(n_samples)
    train_indices = shuffled_indices[:-n_val]
    val_indices = shuffled_indices[-n_val:]
\end{minted}

\section{Neural networks}
\label{sec:org8d5246b}
\subsection{Linear models}
\label{sec:orgc7bf91b}
The \texttt{nn.Linear} constructor takes in \textbf{three} arguments:
\begin{enumerate}
\item Number of input features
\item Number of output features
\item Bias (True or False)
\end{enumerate}
\begin{minted}[]{python}
    import torch.nn as nn

    linear_model = nn.Linear(1, 1) # bias is 'True' by default
    linear_model(t_un_val)
\end{minted}
The linear module so constructed can now be called like a regular function, by passing in the input matrix as an argument. To determine the parameters of the linear model, we can call the \texttt{parameters} function on \texttt{nn.Module} to return a list of parameters. For instance, in the above linear model, the following can be done to obtain the parameters:
\begin{minted}[]{python}
    list(linear_model.parameters())
\end{minted}

\subsection{Sequential models}
\label{sec:org0753f74}
Multiple \texttt{nn} modules can be concatenated with the help of the \texttt{nn.Sequential} container. If suppose we were to construct a simple neural network containg 2 \emph{hidden} layers with a \texttt{Tanh} activation, the following code snippet would construct the required model:
\begin{minted}[]{python}
    seq_model = nn.Sequential(
                nn.Linear(1, 13),
                nn.Tanh(),
                nn.Linear(13, 1))
\end{minted}
In essence, the model constructed above has 1 input feature which fans out to 13 hidden features which is then passed through a \texttt{Tanh} activation. This combines linearly to give a single output feature.
\end{document}
