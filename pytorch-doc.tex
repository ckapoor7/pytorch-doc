% Created 2022-01-13 Thu 22:12
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\author{Chaitanya Kapoor}
\date{\today}
\title{Pytorch Documentation}
\hypersetup{
 pdfauthor={Chaitanya Kapoor},
 pdftitle={Pytorch Documentation},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

A list of important stuff in \textbf{PyTorch} that I do not want to keep using the official documentation for. I have made this using the book \emph{``Deep Learning with PyTorch''}.

\section{Tensor Manipulation}
\label{sec:org168aa47}
Tensors are indexed similar to multidimensional arrays. A Python list can also be passed to the constructor to create a tensor.
\begin{minted}[]{python}
    points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])
\end{minted}
The shape of the created tensor can also be accessed. Note that \texttt{shape} is \textbf{not} a callable function.
\begin{minted}[]{python}
    points.shape
\end{minted}
Some common ways to index a tensor (analogous to lists):
\begin{minted}[]{python}
    points[1:]      # all rows after the first (implicitly all columns)
    points[1:, :]   # all columns
    points[1:, 0]   # first column
    points[None]    # add a dimension of size 1 (like unsqueeze)
\end{minted}

\subsection{Tensor dimensions}
\label{sec:orgbddd27f}
3D tensors have shapes in the following order:
\begin{minted}[]{python}
    img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]
\end{minted}
A 4D image tensor has the following shape:
\begin{minted}[]{python}
    batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]
\end{minted}
RGB dimensions are always counted third from the end, \textbf{-3}.

\subsection{In-place operations}
\label{sec:org4201e3d}
Methods which are specific to \texttt{Tensor} objects are followed by an \emph{underscore}. This is used to indicate that the method operates \emph{in place} by modifying the input instead of creating a new tensor.
\begin{minted}[]{python}
    a = torch.ones(3,2)
    a.zero_()
\end{minted}
This code block mutates \texttt{a} and returns a null tensor of shape \((3,2)\).

\subsection{Move to GPUs}
\label{sec:org3c99ceb}
A tensor can be created on the GPU by providing the following argument to the constructor:
\begin{minted}[]{python}
    points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')
\end{minted}
We can also copy a tensor created on the CPU to the GPU with the help of the \texttt{to} method.
\begin{minted}[]{python}
    points_gpu = points.to(device='cuda')
\end{minted}
Basic tensor operations can also be transferred to the GPU rather than performing them on the CPU.
\begin{minted}[]{python}
    points_gpu = 2 * points.to(device='cuda')
\end{minted}

\subsection{Serializing tensors}
\label{sec:orgb4eb220}
To avoid retraining models from scratch, we can save weight tensors with the help of the \texttt{pickle} format. If we have an output file called \texttt{ourpoints.t}, then:
\begin{minted}[]{python}
    torch.save(points, '../ourpoints.t')
\end{minted}
To load the weights, we call the \texttt{load} method.
\begin{minted}[]{python}
    points = torch.load('..ourpoints.t')
\end{minted}

\section{Images}
\label{sec:orgc3daad2}
\subsection{Loading images}
\label{sec:orgd341c7b}
This can be done with the \texttt{imageio} library. Suppose that the path to the image file is \texttt{'../xyz.jpg'}:
\begin{minted}[]{python}
    import imageio
    img_arr = imageio.imread('../xyz.jpg')
\end{minted}
The image is loaded as a \texttt{NumPy} object. The layout of the dimensions is \(H\times W\times C\) corresponding to \textbf{height}, \textbf{width} and \textbf{channel} respectively.
\subsection{Changing layout}
\label{sec:org2849981}
The layout of the image tensor can be altered using the \texttt{permute} method (as expected by \texttt{Pytorch}).
\begin{minted}[]{python}
    torch_img = torch.from_numpy(img_arr) # convert to torch tensor
    out = torch_img.permute(2, 0, 1) # (C X H X W)
\end{minted}
An extra dimension can be added for processing batches of images by specifying the dimensions of the tensor as \(N\times C\times H\times W\) where \(N\) is the batch size of the images.

\section{Learning process}
\label{sec:orge811523}
\subsection{Visualizing}
\label{sec:orgb65fb92}
The most foolproof method to debug code is to begin by plotting it out. In the following code snippet, we plot \emph{raw unknown values}.
\begin{minted}[]{python}
    %matplotlib inline
    from matplotlib import pyplot as plt

    t_p = model(t_un, *params) # argument unpacking
    fig = plt.figure(dpi=600)
    plt.xlabel("Temperature (°Fahrenheit)")
    plt.ylabel("Temperature (°Celsius)")
    plt.plot(t_u.numpy(), t_p.detach().numpy())
    plt.plot(t_u.numpy(), t_c.numpy(), 'o')
\end{minted}
The argument \texttt{*params} is equivalent to passing the elements of \texttt{params} as individual arguemnts to our training model.

\subsection{Backpropagation}
\label{sec:orgb2be1ce}
A training loop can be written with the help of gradient descent optimizers and standard loss functions that are shipped along with the \texttt{PyTorch} packages. The following code block illustrates a training loop function:
\begin{minted}[]{python}
    def training_loop(n_epochs, optimizer, params, t_u, t_c):
        for epoch in range(1, n_epochs + 1):
            t_p = model(t_u, *params)
            loss = loss_fn(t_p, t_c) # custom loss function
            optimizer.zero_grad()
            loss.backward()
            optimizer.step() # update parameters
            if epoch % 500 == 0:
                print('Epoch %d, Loss %f' % (epoch, float(loss)))
        return params
\end{minted}
Note that the gradients are \emph{zeroed out} after each loss update to ensure that the gradients are not accumulated in the leaf nodes of the computation graph that is constructed. An example of invoking the above training method using the \texttt{SGD} optimizer is shown below.
\begin{minted}[]{python}
    params = torch.tensor([1.0, 0.0], requires_grad=True)
    learning_rate = 1e-2
    optimizer = optim.SGD([params], lr=learning_rate)
    training_loop(
        n_epochs = 5000,
        optimizer = optimizer,
        params = params,
        t_u = t_un,
        t_c = t_c)
\end{minted}

\subsection{Splitting datasets}
\label{sec:org4feedc2}
Shuffling a dataset requires us to use a random permutation of tensor indices. This is done with the help of the \texttt{randperm} function.
\begin{minted}[]{python}
    n_samples = t_u.shape[0]
    n_val = int(0.2 * n_samples)   # perform an 80:20 split
    shuffled_indices = torch.randperm(n_samples)
    train_indices = shuffled_indices[:-n_val]
    val_indices = shuffled_indices[-n_val:]
\end{minted}

\section{Neural networks}
\label{sec:org718d6eb}
\subsection{Linear models}
\label{sec:org7591158}
The \texttt{nn.Linear} constructor takes in \textbf{three} arguments:
\begin{enumerate}
\item Number of input features
\item Number of output features
\item Bias (True or False)
\end{enumerate}
\begin{minted}[]{python}
    import torch.nn as nn

    linear_model = nn.Linear(1, 1) # bias is 'True' by default
    linear_model(t_un_val)
\end{minted}
The linear module so constructed can now be called like a regular function, by passing in the input matrix as an argument. To determine the parameters of the linear model, we can call the \texttt{parameters} function on \texttt{nn.Module} to return a list of parameters. For instance, in the above linear model, the following can be done to obtain the parameters:
\begin{minted}[]{python}
    list(linear_model.parameters())
\end{minted}

\subsection{Sequential models}
\label{sec:orgcf75769}
Multiple \texttt{nn} modules can be concatenated with the help of the \texttt{nn.Sequential} container. If suppose we were to construct a simple neural network containg 2 \emph{hidden} layers with a \texttt{Tanh} activation, the following code snippet would construct the required model:
\begin{minted}[]{python}
    seq_model = nn.Sequential(
                nn.Linear(1, 13),
                nn.Tanh(),
                nn.Linear(13, 1))
\end{minted}
In essence, the model constructed above has 1 input feature which fans out to 13 hidden features which is then passed through a \texttt{Tanh} activation. This combines linearly to give a single output feature.

\section{Basic vision}
\label{sec:org5dcb252}
\subsection{Datasets}
\label{sec:org9c37bc9}
\subsubsection{Download}
\label{sec:org9ac6975}
We make use of the \texttt{datasets} module to download standard datasets. For example, we would download the \textbf{CIFAR-10} dataset in the following manner:
\begin{minted}[]{python}
    from torchvision import datasets
    data_path = '/path/to/download' # download to this directory path
    cifar10 = datasets.CIFAR10(data_path, train=True,
                               download=True) # training data
    cifar10_val = datasets.CIFAR10(data_path, train=False,
                                   download=True) # validation data
\end{minted}

\subsubsection{Dataset class}
\label{sec:orgd928971}
The length of the dataset object created above can be determined using \texttt{len}, which is a built-in Python function.
\begin{minted}[]{python}
    len(cifar10)
\end{minted}
Similarly, we can use standard subscripting as done for tuples and lists to access individual elements of the dataset.
\begin{minted}[]{python}
    img, label = cifar10[99] # 99th image of the dataset
\end{minted}

\subsubsection{Transform}
\label{sec:org53b57e7}
In most cases, we would need to convert images which are in the form of PIL images or NumPy arrays to tensors.
\begin{minted}[]{python}
    from torchvision import transforms
    to_tensor = transforms.ToTensor()
    img_t = to_tensor(img)
\end{minted}
\texttt{img\_t} is laid out in the format \(C\times H\times W\). For the sake of brevity, this can also be passed directly as an argument to \texttt{dataset.CIFAR10}:
\begin{minted}[]{python}
    tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,
                                      transform=transforms.ToTensor())
\end{minted}

\subsubsection{Normalize}
\label{sec:org217902b}
Suppose that we wanted to normalize the CIFAR10 dataset. To do so, we need to compute the mean as well as standard deviation per channel. We begin by stacking all image tensors along an extra dimension.
\begin{minted}[]{python}
    imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)
\end{minted}
And now, we are in a position to compute the desired values.
\begin{minted}[]{python}
    imgs.view(3, -1).mean(dim=1) # 3X1024 vector
    imgs.view(3, -1).std(dim=1) # standard deviation
\end{minted}
The argument for normalization can then be added to \texttt{datasets.CIFAR10} with the help of the \texttt{transforms.Normalize} function by giving the mean and standard deviation as arguments to the latter.

\subsection{Dataloaders}
\label{sec:org0088117}
The \texttt{DataLoader} class helps in shuffling the data and organizing it in minibatches. A sample training loop for it looks like the code snippet below.
\begin{minted}[]{python}
    import torch
    import torch.nn as nn

    train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
                                            shuffle=True)

    model = nn.Sequential(
                nn.Linear(3072, 512),
                nn.Tanh(),
                nn.Linear(512, 2),
                nn.LogSoftmax(dim=1))

    learning_rate = 1e-2

    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    loss_fn = nn.NLLLoss()
    n_epochs = 100

    for epoch in range(n_epochs):
        for imgs, labels in train_loader:
            batch_size = imgs.shape[0]
            outputs = model(imgs.view(batch_size, -1))
            loss = loss_fn(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
\end{minted}
\end{document}
