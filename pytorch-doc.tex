% Created 2022-01-03 Mon 15:32
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\author{Chaitanya Kapoor}
\date{\today}
\title{Pytorch Documentation}
\hypersetup{
 pdfauthor={Chaitanya Kapoor},
 pdftitle={Pytorch Documentation},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

A list of important stuff in \textbf{PyTorch} that I do not want to keep using the official documentation for. I have made this using the book \emph{``Deep Learning with PyTorch''}.

\section{Tensor Manipulation}
\label{sec:orgb3c2440}
Tensors are indexed similar to multidimensional arrays. A Python list can also be passed to the constructor to create a tensor.
\begin{minted}[]{python}
points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])
\end{minted}
The shape of the created tensor can also be accessed. Note that \texttt{shape} is \textbf{not} a callable function.
\begin{minted}[]{python}
points.shape
\end{minted}
Some common ways to index a tensor (analogous to lists):
\begin{minted}[]{python}
points[1:]      # all rows after the first (implicitly all columns)
points[1:, :]   # all columns
points[1:, 0]   # first column
points[None]    # add a dimension of size 1 (like unsqueeze)
\end{minted}

\subsection{Tensor dimensions}
\label{sec:orga6a3f88}
3D tensors have shapes in the following order:
\begin{minted}[]{python}
img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]
\end{minted}
A 4D image tensor has the following shape:
\begin{minted}[]{python}
batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]
\end{minted}
RGB dimensions are always counted third from the end, \textbf{-3}.

\subsection{In-place operations}
\label{sec:org97dc598}
Methods which are specific to \texttt{Tensor} objects are followed by an \emph{underscore}. This is used to indicate that the method operates \emph{in place} by modifying the input instead of creating a new tensor.
\begin{minted}[]{python}
a = torch.ones(3,2)
a.zero_()
\end{minted}
This code block mutates \texttt{a} and returns a null tensor of shape \((3,2)\).

\subsection{Move to GPUs}
\label{sec:org5fae4ff}
A tensor can be created on the GPU by providing the following argument to the constructor:
\begin{minted}[]{python}
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')
\end{minted}
We can also copy a tensor created on the CPU to the GPU with the help of the \texttt{to} method.
\begin{minted}[]{python}
points_gpu = points.to(device='cuda')
\end{minted}
Basic tensor operations can also be transferred to the GPU rather than performing them on the CPU.
\begin{minted}[]{python}
points_gpu = 2 * points.to(device='cuda')
\end{minted}

\subsection{Serializing tensors}
\label{sec:orgac39d48}
To avoid retraining models from scratch, we can save weight tensors with the help of the \texttt{pickle} format. If we have an output file called \texttt{ourpoints.t}, then:
\begin{minted}[]{python}
torch.save(points, '../ourpoints.t')
\end{minted}
To load the weights, we call the \texttt{load} method.
\begin{minted}[]{python}
points = torch.load('..ourpoints.t')
\end{minted}
\end{document}
