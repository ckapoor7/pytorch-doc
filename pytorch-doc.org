#+TITLE: Pytorch Documentation
#+AUTHOR: Chaitanya Kapoor


A list of important stuff in *PyTorch* that I do not want to keep using the official documentation for. I have made this using the book /"Deep Learning with PyTorch"/.

* Tensor Manipulation
Tensors are indexed similar to multidimensional arrays. A Python list can also be passed to the constructor to create a tensor.
#+begin_src python
points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])
#+end_src
The shape of the created tensor can also be accessed. Note that =shape= is *not* a callable function.
#+begin_src python
points.shape
#+end_src
Some common ways to index a tensor (analogous to lists):
#+begin_src python
points[1:]      # all rows after the first (implicitly all columns)
points[1:, :]   # all columns
points[1:, 0]   # first column
points[None]    # add a dimension of size 1 (like unsqueeze)
#+end_src

** Tensor dimensions
3D tensors have shapes in the following order:
#+begin_src python
img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]
#+end_src
A 4D image tensor has the following shape:
#+begin_src python
batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]
#+end_src
RGB dimensions are always counted third from the end, *-3*.

** In-place operations
Methods which are specific to =Tensor= objects are followed by an /underscore/. This is used to indicate that the method operates /in place/ by modifying the input instead of creating a new tensor.
#+begin_src python
a = torch.ones(3,2)
a.zero_()
#+end_src
This code block mutates =a= and returns a null tensor of shape $(3,2)$.

** Move to GPUs
A tensor can be created on the GPU by providing the following argument to the constructor:
#+begin_src python
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')
#+end_src
We can also copy a tensor created on the CPU to the GPU with the help of the =to= method.
#+begin_src python
points_gpu = points.to(device='cuda')
#+end_src
Basic tensor operations can also be transferred to the GPU rather than performing them on the CPU.
#+begin_src python
points_gpu = 2 * points.to(device='cuda')
#+end_src

** Serializing tensors
To avoid retraining models from scratch, we can save weight tensors with the help of the =pickle= format. If we have an output file called =ourpoints.t=, then:
#+begin_src python
torch.save(points, '../ourpoints.t')
#+end_src
To load the weights, we call the =load= method.
#+begin_src python
points = torch.load('..ourpoints.t')
#+end_src

* Images
** Loading images
This can be done with the =imageio= library. Suppose that the path to the image file is ='../xyz.jpg'=:
#+begin_src python
    import imageio
    img_arr = imageio.imread('../xyz.jpg')
#+end_src
The image is loaded as a =NumPy= object. The layout of the dimensions is $H\times W\times C$ corresponding to *height*, *width* and *channel* respectively.
** Changing layout
The layout of the image tensor can be altered using the =permute= method (as expected by =Pytorch=).
#+begin_src python
 torch_img = torch.from_numpy(img_arr) # convert to torch tensor
 out = torch_img.permute(2, 0, 1) # (C X H X W)
#+end_src
An extra dimension can be added for processing batches of images by specifying the dimensions of the tensor as $N\times C\times H\times W$ where $N$ is the batch size of the images.

* Learning process
** Visualizing
The most foolproof method to debug code is to begin by plotting it out. In the following code snippet, we plot /raw unknown values/.
#+begin_src python
    %matplotlib inline
    from matplotlib import pyplot as plt

    t_p = model(t_un, *params) # argument unpacking
    fig = plt.figure(dpi=600)
    plt.xlabel("Temperature (°Fahrenheit)")
    plt.ylabel("Temperature (°Celsius)")
    plt.plot(t_u.numpy(), t_p.detach().numpy())
    plt.plot(t_u.numpy(), t_c.numpy(), 'o')
#+end_src
The argument =*params= is equivalent to passing the elements of =params= as individual arguemnts to our training model.

** Backpropagation
A training loop can be written with the help of gradient descent optimizers and standard loss functions that are shipped along with the =PyTorch= packages. The following code block illustrates a training loop function:
#+begin_src python
    def training_loop(n_epochs, optimizer, params, t_u, t_c):
        for epoch in range(1, n_epochs + 1):
            t_p = model(t_u, *params)
            loss = loss_fn(t_p, t_c) # custom loss function
            optimizer.zero_grad()
            loss.backward()
            optimizer.step() # update parameters
            if epoch % 500 == 0:
                print('Epoch %d, Loss %f' % (epoch, float(loss)))
        return params
#+end_src
Note that the gradients are /zeroed out/ after each loss update to ensure that the gradients are not accumulated in the leaf nodes of the computation graph that is constructed. An example of invoking the above training method using the =SGD= optimizer is shown below.
#+begin_src python
    params = torch.tensor([1.0, 0.0], requires_grad=True)
    learning_rate = 1e-2
    optimizer = optim.SGD([params], lr=learning_rate)
    training_loop(
        n_epochs = 5000,
        optimizer = optimizer,
        params = params,
        t_u = t_un,
        t_c = t_c)
#+end_src

** Splitting datasets
Shuffling a dataset requires us to use a random permutation of tensor indices. This is done with the help of the =randperm= function.
#+begin_src python
    n_samples = t_u.shape[0]
    n_val = int(0.2 * n_samples)   # perform an 80:20 split
    shuffled_indices = torch.randperm(n_samples)
    train_indices = shuffled_indices[:-n_val]
    val_indices = shuffled_indices[-n_val:]
#+end_src
